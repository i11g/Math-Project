{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c53bbb-848c-4b01-b83b-f15b5a1e7c46",
   "metadata": {},
   "source": [
    "# Comparative Study of Dynamic Programming and Genetic Algorithms for the Solution of 0/1 Knapsack Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07fd39-f7ee-4379-b9e8-dc309824a8d9",
   "metadata": {},
   "source": [
    "### Author Ivan Georgiev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09809a2e-f08d-4a38-b6f2-019bb67ce9e8",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "The 0/1 Knapsack Problem, involves selecting a subset of items with given weights and values to maximize total value without exceeding a capacity limit. This project aims to compares two distinct approaches to solving this challange: Dynamic Programming (DP), which guarantees an optimal solution, and Genetic Algorithms (GA), a heuristic method inspired by natural evolution. By implementing both algorithms in Python, we evaluate their performance in terms of runtime and solution quality across varying problem sizes. Experiments reveal that DP excels in smaller instances with its precision, while GA offers scalability for larger instances at the cost of optimality. These findings highlight the trade-offs between exact and approximate methods, providing insights into their practical applicability in resource allocation and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaad1e-242c-4ff4-831f-e6f424fd45e3",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The 0/1 Knapsack Problem is a fundamental challenge within the field of combinatorial optimization. The problem is related to the following: in a given set of items, with a different weight and values, the task is to select the most valuable subset of these items that can be accommodated within a knapsack with a specific capacity. The stipulation is that each item can be either entirely excluded or included in the subset and that's what 0/1 means in the name of the challenge.   \n",
    "The problem is very important in theory and practice and can find numerous implications including efficient budget allocations in budget constraints situations, optimization solutions in transportation, logistics and cargo, and even applications in the fields of projects planning and resources distribution.    \n",
    "The well-known solution of the problem involves the Dynamic Programing(DP) algorithm. The algorithm's strengths are based in its systematic methodology related to the principles of optimal substructure and overlapping subproblems. It will always find the optimal solution but the pseudo-polynomial time complexity related to the DP algorithm $O(nW)$ (where $n$ is number of items and $W$ is the capacity) can lead to computational inefficiencies when the number of items or the knapsack capacity become too large. \n",
    "In that perspective, the challenge when the number of knapsack instances becomes very large can be tackled using another approach called Genetic Algorithms(GAs). Those algorithms present a heuristic approach of solving the problem, inspired from the mechanisms of natural evolution. The approach can often yield good to near-optimal solutions in a more efficient way, especially when dealing with large or more intricate instances. While GAs do not guarantee an optimal solution, they often achieve near-optimal results much faster than exact algorithms like DP. \n",
    "In this project, our goal is to compare DP and GA in the context of 0-1 Knapsack Problem in terms of runtime performance and solution quality using different numbers of items. Specifically, we want to observe how their runtimes scale with problem size and how close the GA‚Äôs solution value is to the optimal value found by DP. The basic of the comparison is the understanding of the inherent trade-offs between these two methodologies: DP offers optimality and a deterministic path to the solution, while GA provides flexibility and speed, particularly when the computational demands of exact methods become excessive. \n",
    "## Theoretical Background \n",
    "### 0/1 Knapsack Problem Definition \n",
    "\n",
    "Formally, we have $n$ items, each item $i$ with weight $w_i$ and value $v_i$, and a knapsack with capacity $W$. We define binary decision variables\n",
    "$$\n",
    "x_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if we include item }i,\\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\max\\;& \\sum_{i=1}^n v_i x_i \\quad (\\text{maximize total value})\\\\\n",
    "\\text{s.t.}\\;& \\sum_{i=1}^n w_i x_i \\le W \\quad (\\text{weight capacity})\\\\\n",
    "& x_i \\in \\{0,1\\}, \\quad i = 1,\\dots,n.\n",
    "\\end{align*} $$\n",
    "The goal is to select a subset of the items such that the total weight does not exceed $W$, and the total value is as large as possible. Because each $x_i$ is restricted to 0 or 1 (we either take an item or not, with no fractional quantities), this is the 0-1 knapsack problem.\n",
    "This problem is NP-hard, and the decision version (‚ÄúCan we achieve at least a value $V$ without exceeding weight $W$?‚Äù) is NP-complete.  For this reason, algorithms that always find the optimal solution (like brute force or DP) have worst-case exponential runtime or pseudo-polynomial runtime.\n",
    "\n",
    "###  Dynamic Programming Algorithm Overview \n",
    "\n",
    "Dynamic Programming (DP) is a powerful algorithmic paradigm for solving complex problems by breaking them down into simpler overlapping subproblems. At its heart lies two key principles:\n",
    "\n",
    "- Optimal Substructure\n",
    "\n",
    "- Overlapping Subproblems\n",
    "\n",
    "It is a method for solving problems by combining the solutions of subproblems, storing (‚Äúmemoizing‚Äù) those solutions to avoid redundant work. Richard Bellman, who coined the term in the 1950s, formalized it through the Bellman Equation: if $ ùëâ (ùë•) $ is the optimal value for state \n",
    "$ ùë• $, and you can make a decision $ ùë¢ $transitioning $ x ‚Üí ùë¶ $ with cost $c(ùë•,ùë¢,ùë¶)$, then \n",
    "\n",
    "$$ V(x) = \\min_{u} \\bigl\\{\\,c(x,u,y) + V(y)\\bigr\\}.$$\n",
    "\n",
    "This recurrence captures both the subproblem decomposition and the optimization criterion.\n",
    "\n",
    "Dynamic Programming provides an exact solution by breaking the problem into subproblems. Let $ dp[i][w] $ represent the maximum value that can be obtained by considering items up to index $ i $ (from 0 to n-1) with a knapsack capacity of $ w $ (from 0 to W). The recurrence relation is defined as follows $1 \\le i \\le n$ and $0 \\le w \\le W$: \n",
    "\n",
    "- If we do not take item $i$, the value remains $ DP[i-1][w] $ (the optimum without this item).\n",
    "\n",
    "\n",
    "- If we take item $i$ (of weight $w_i$ and value $v_i$), then we gain value $v_i$ plus whatever the best we can do with remaining capacity $w-w_i$ using previous items, which is $ DP [i-1][w-w-1]$ This is only feasible if $w_i \\le w$.\n",
    "\n",
    "So, we have the following two DP transitions:  \n",
    "\n",
    "$$ DP[i][w] =\n",
    "\\begin{cases}\n",
    "\\max\\bigl(DP[i-1][w],\\,DP[i-1][w-w_i] + v_i\\bigr), & w_i \\le w,\\\\\n",
    "DP[i-1][w], & w_i > w.\n",
    "\\end{cases} $$ \n",
    "\n",
    "Taking the better of these two choices yields the DP transition: \n",
    "\n",
    "$$ DP[i][w] = \\max\\bigl(DP[i-1][w],\\,DP[i-1][w-w_i] + v_i\\bigr)\n",
    "\\quad\\text{if }w_i \\le w. $$  \n",
    "\n",
    "The base cases are $DP[0][w] = 0$ for all $w$ (with no items, value is 0) and $DP[i][0] = 0$ for all $i$ (with zero capacity, we can take nothing).\n",
    "\n",
    "The DP solution is typically implemented using a bottom-up approach. A 2D table, often denoted as $ dp $, of size (n+1) x (W+1) is constructed. The rows of the table represent the number of items considered (from 0 to n), and the columns represent the knapsack capacity (from 0 to W). The table is filled iteratively, starting from the base cases where no items are considered (dp[w] = 0 for all w) or when the knapsack has zero capacity (dp[i] = 0 for all i).8 The values in the table are computed using the recurrence relation described above, progressing from smaller subproblems to the final solution, which is found at dp[n][W].\n",
    "\n",
    "\n",
    "###  Genetic Algorithm \n",
    "\n",
    "A Genetic Algorithm (GA) is a probabilistic, evolutionary heuristic approach that searches for good solutions by imitating the process of natural selection. Inspired by the principles of natural selection and genetics, GAs are often employed to find near-optimal solutions for complex optimization problems, particularly when the search space is vast and finding an exact optimal solution is computationally infeasible.\n",
    "\n",
    "A GA starts with a population of candidate solutions (individuals), each represented as a chromosome (e.g., a string of bits, numbers, or symbols). These solutions evolve over generations to optimize a fitness function $ f(x) $ which quantifies how good a solution is. \n",
    "#### Key Steps\n",
    "1. Initialize a population.\n",
    "2. Evaluate fitness of each individual.\n",
    "3. Select parents based on fitness.\n",
    "4. Apply crossover to create offspring.\n",
    "5. Apply mutation to introduce randomness.\n",
    "6. Replace the population (partially or fully) with offspring.\n",
    "7. Repeat until a termination condition (e.g., max generations or satisfactory fitness) is met.<br>\n",
    "#### Population Initialization\n",
    "  - A population of size $ ùëÅ $ is initialized, where each individual $ x_i$ $(i = 1,2,\\dots,N)$ is a candidate solution.\n",
    "  - Chromosomes are often encoded as:\n",
    "   - Binary strings (e.g. x_i = [0,1,1,0])\n",
    "   - Real numbers (e.g. x_i = [3.14, 2.71])\n",
    "  - Initialization is typically random within a defined search space.<br>\n",
    "#### Fitness Function\n",
    "- The fitness function $ f(x_i) $ evaluates the quality of the individual $ x_i $.\n",
    "- For maximization problems, higher $ f(x_i) $ indicates a better solution.\n",
    "- For minimization problems, transform the objective, e.g., $f(x_i) = \\frac{1}{1 + g(x_i)}$, where $ g(x_i) $ is the cost function.\n",
    "- Example: If optimizing $g(x)=x^2$, the fitness might be $ f(x)\\frac{1}{1+x^2}$.\n",
    "#### Selection\n",
    "- Selection chooses individuals to reproduce based on their fitness. Candidates are selected for reproduction based on their fitness, favoring those with higher fitness (the principle of \"survival of the fittest\").\n",
    "- There are two common selection methods:\n",
    "  - Fitness Proportionate Selection (Roulette Wheel):\n",
    "     - Probability of selecting the individual $ x_i$\n",
    "        $$ P(x_i) = \\sum_{j=1}^N \\frac {f(x_j)}{fx_i} \n",
    "     - To spin the wheel a cumulative probability is used.\n",
    "  - Tournament Selection\n",
    "    - Randomly pick $ k $ individuals and select the one with the highest fitness\n",
    "    - Probability depends on fitness rankings\n",
    "  - Rank-Based Selection\n",
    "    - Assign selection probabilities based on fitness rank rather than row fitness values.\n",
    "#### Crossover (Recombination) \n",
    "- Crossover combines the genetic materials of two parents to produce offsprings.\n",
    "- From two parents $ x_1$ and $X_2$ crossover occurs with probability $ p_c$ (typically 0.6-0.9).\n",
    "- Common crossover methods:\n",
    "  - Single-Point Crossover\n",
    "    - Choose random point $ k $ \n",
    "\n",
    "The GA evolves the population over a number of iterations called generations. In each generation, the following genetic operators are applied:\n",
    "-  We use tournament selection, where a few random individuals compete and the one with the highest fitness wins the chance to reproduce.\n",
    "- Crossover: Also known as recombination, crossover takes two parent solutions and combines them to produce one or two offspring. A common method is single-point crossover: a random cut point is chosen in the chromosome string, and the prefix of Parent A is combined with the suffix of Parent B to form one child, while the complementary pieces form the second child. The idea is to mix genetic information from two good solutions, hoping to create an even better solution. Crossover is typically applied with a certain probability (crossover rate); if no crossover occurs, offspring are just copies of the parents.\n",
    "- Mutation: After crossover, a random mutation is applied to each offspring with a small probability. Mutation flips some bits (from 0 to 1 or vice versa) in the chromosome. This introduces new genetic diversity into the population, which helps the algorithm avoid getting stuck in local optima. For example, a mutation might add or remove a random item from the knapsack. The mutation rate controls how often these random changes occur (e.g. 5% chance for each bit).\n",
    "- Elitism (Replacement): Often, GAs will carry over the best individuals from one generation to the next to ensure the quality does not decrease. In our approach, we preserve the single best solution found so far (elitism of size 1) by automatically carrying it into the next generation. \n",
    "\n",
    "To solve the 0/1 Knapsack problem we maintain a population of candidate solutions (often encoded as binary strings for the 0-1 knapsack). Each candidate (called a chromosome) is essentially a vector of 0/1 of length $n$, where each bit represents whether a particular item is included (1) or not (0).\n",
    "We also define a fitness function to evaluate the quality of each candidate solution. For the knapsack problem, a natural fitness is the total value of selected items. However, candidates that violate the weight constraint (i.e. total weight > $W$) should be penalized or considered infeasible. In our implementation, we assign a very low fitness (even negative) to overweight solutions to discourage them, ensuring that only or mostly feasible solutions persist. \n",
    "This evolutionary loop of selection, crossover, mutation, and replacement is repeated for many generations. Over time, we expect the population to ‚Äúevolve‚Äù better solutions ‚Äì the average fitness should increase, and the best solution in the population should approach the optimal. The process terminates either after a fixed number of generations or when improvements stagnate. GAs are stochastic by nature: different runs may produce different results, and they do not guarantee an optimal solution, but they often find a very good solution in a fraction of the time required by exact methods. It is important to highlight that the GA is a probabilistic algorithm, meaning that it does not guarantee finding the absolute optimal solution. However, it often proves effective in finding good, near-optimal solutions within a reasonable timeframe, particularly for large and complex problem instances where exact methods like DP might be computationally infeasible. \n",
    "\n",
    "###  Previous Work Preview  \n",
    "\n",
    "Numerous academic studies have been conducted to compare heuristic approaches, such as Genetic Algorithms, with exact methods, like Dynamic Programming, for solving NP-hard problems, with a particular focus on the 0/1 Knapsack Problem. These comparisons aim to understand the strengths and weaknesses of each approach under different problem conditions. Findings from these studies generally indicate that DP is more efficient for small to medium-sized instances, especially when an optimal solution is required. For example, research suggests that DP excels when the number of items and the knapsack capacity are within manageable limits, providing a guaranteed optimal solution. However, these studies also point out that DP's runtime tends to grow polynomially with the number of items but can increase more significantly with the capacity, potentially becoming impractical for very large capacities. Conversely, previous work has shown that GA can be more effective for larger instances of the 0/1 Knapsack Problem, often delivering near-optimal solutions in significantly shorter time compared to DP. Studies indicate that while GA might not always find the absolute best solution, its ability to explore a large solution space efficiently allows it to quickly converge to a high-quality solution, especially when the problem size makes DP computationally expensive. Some research suggests that GA's runtime is less sensitive to the knapsack capacity and more influenced by the number of items and the number of generations it runs. Furthermore, the literature also includes explorations of hybrid approaches that combine DP and GA to leverage the advantages of both methodologies. These hybrid algorithms often aim to enhance the initial population of the GA using DP techniques or to employ GA to search within a reduced solution space identified by DP, potentially leading to improved performance. Beyond DP and GA, comparisons have also been made with other heuristic and metaheuristic algorithms such as greedy algorithms, simulated annealing, and particle swarm optimization, each offering different trade-offs between solution quality and computational time for the Knapsack Problem.\n",
    "\n",
    "In other words the existing body of research strongly supports our initial hypothesis. The consistent findings across various studies provide a solid foundation for the experimental design and analysis proposed in this project. The recurring theme in the literature highlights the expected strengths of DP for smaller problem instances where optimality is paramount and the computational cost is manageable, as well as the potential of GA to provide efficient, high-quality solutions for larger instances where DP becomes less practical due to its computational demands. The exploration of hybrid algorithms further suggests an ongoing interest in optimizing the solution process for the Knapsack Problem by combining the benefits of different algorithmic paradigms. This context underscores the value of the current comparative study in further elucidating the conditions under which each algorithm is most effective.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6156f454-1a33-486e-b630-617ff06cac7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "522d7b02-81ca-4259-b840-f2857d34fbe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
