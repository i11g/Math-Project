{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92c53bbb-848c-4b01-b83b-f15b5a1e7c46",
   "metadata": {},
   "source": [
    "# Comparative Study of Dynamic Programming and Genetic Algorithms for the Solution of 0/1 Knapsack Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b07fd39-f7ee-4379-b9e8-dc309824a8d9",
   "metadata": {},
   "source": [
    "### Author Ivan Georgiev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09809a2e-f08d-4a38-b6f2-019bb67ce9e8",
   "metadata": {},
   "source": [
    "## Abstract \n",
    "\n",
    "The 0/1 Knapsack Problem, involves selecting a subset of items with given weights and values to maximize total value without exceeding a capacity limit. This project aims to compares two distinct approaches to solving this challange: Dynamic Programming (DP), which guarantees an optimal solution, and Genetic Algorithms (GA), a heuristic method inspired by natural evolution. By implementing both algorithms in Python, we evaluate their performance in terms of runtime and solution quality across varying problem sizes. Experiments reveal that DP excels in smaller instances with its precision, while GA offers scalability for larger instances at the cost of optimality. These findings highlight the trade-offs between exact and approximate methods, providing insights into their practical applicability in resource allocation and beyond."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffaad1e-242c-4ff4-831f-e6f424fd45e3",
   "metadata": {},
   "source": [
    "## Introduction \n",
    "\n",
    "The 0/1 Knapsack Problem is a fundamental challenge within the field of combinatorial optimization. The problem is related to the following: in a given set of items, with a different weight and values, the task is to select the most valuable subset of these items that can be accommodated within a knapsack with a specific capacity. The stipulation is that each item can be either entirely excluded or included in the subset and that's what 0/1 means in the name of the challenge.   \n",
    "The problem is very important in theory and practice and can find numerous implications including efficient budget allocations in budget constraints situations, optimization solutions in transportation, logistics and cargo, and even applications in the fields of projects planning and resources distribution.    \n",
    "The well-known solution of the problem involves the Dynamic Programing(DP) algorithm. The algorithm's strengths are based in its systematic methodology related to the principles of optimal substructure and overlapping subproblems. It will always find the optimal solution but the pseudo-polynomial time complexity related to the DP algorithm $O(nW)$ (where $n$ is number of items and $W$ is the capacity) can lead to computational inefficiencies when the number of items or the knapsack capacity become too large. \n",
    "In that perspective, the challenge when the number of knapsack instances becomes very large can be tackled using another approach called Genetic Algorithms(GAs). Those algorithms present a heuristic approach of solving the problem, inspired from the mechanisms of natural evolution. The approach can often yield good to near-optimal solutions in a more efficient way, especially when dealing with large or more intricate instances. While GAs do not guarantee an optimal solution, they often achieve near-optimal results much faster than exact algorithms like DP. \n",
    "In this project, our goal is to compare DP and GA in the context of 0-1 Knapsack Problem in terms of runtime performance and solution quality using different numbers of items. Specifically, we want to observe how their runtimes scale with problem size and how close the GA‚Äôs solution value is to the optimal value found by DP. The basic of the comparison is the understanding of the inherent trade-offs between these two methodologies: DP offers optimality and a deterministic path to the solution, while GA provides flexibility and speed, particularly when the computational demands of exact methods become excessive. \n",
    "## Theoretical Background \n",
    "### 0/1 Knapsack Problem Definition \n",
    "\n",
    "Formally, we have $n$ items, each item $i$ with weight $w_i$ and value $v_i$, and a knapsack with capacity $W$. We define binary decision variables\n",
    "$$\n",
    "x_i =\n",
    "\\begin{cases}\n",
    "1, & \\text{if we include item }i,\\\\\n",
    "0, & \\text{otherwise.}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "$$ \\begin{align*}\n",
    "\\max\\;& \\sum_{i=1}^n v_i x_i \\quad (\\text{maximize total value})\\\\\n",
    "\\text{s.t.}\\;& \\sum_{i=1}^n w_i x_i \\le W \\quad (\\text{weight capacity})\\\\\n",
    "& x_i \\in \\{0,1\\}, \\quad i = 1,\\dots,n.\n",
    "\\end{align*} $$\n",
    "The goal is to select a subset of the items such that the total weight does not exceed $W$, and the total value is as large as possible. Because each $x_i$ is restricted to 0 or 1 (we either take an item or not, with no fractional quantities), this is the 0-1 knapsack problem.\n",
    "This problem is NP-hard, and the decision version (‚ÄúCan we achieve at least a value $V$ without exceeding weight $W$?‚Äù) is NP-complete.  For this reason, algorithms that always find the optimal solution (like brute force or DP) have worst-case exponential runtime or pseudo-polynomial runtime.\n",
    "\n",
    "###  Dynamic Programming Algorithm Overview \n",
    "\n",
    "Dynamic Programming (DP) is a powerful algorithmic paradigm for solving complex problems by breaking them down into simpler overlapping subproblems. At its heart lies two key principles:\n",
    "\n",
    "- Optimal Substructure\n",
    "\n",
    "- Overlapping Subproblems\n",
    "\n",
    "It is a method for solving problems by combining the solutions of subproblems, storing (‚Äúmemoizing‚Äù) those solutions to avoid redundant work. Richard Bellman, who coined the term in the 1950s, formalized it through the Bellman Equation: if $ ùëâ (ùë•) $ is the optimal value for state \n",
    "$ ùë• $, and you can make a decision $ ùë¢ $transitioning $ x ‚Üí ùë¶ $ with cost $c(ùë•,ùë¢,ùë¶)$, then \n",
    "\n",
    "$$ V(x) = \\min_{u} \\bigl\\{\\,c(x,u,y) + V(y)\\bigr\\}.$$\n",
    "\n",
    "This recurrence captures both the subproblem decomposition and the optimization criterion.\n",
    "\n",
    "Dynamic Programming provides an exact solution by breaking the problem into subproblems. Let $ dp[i][w] $ represent the maximum value that can be obtained by considering items up to index $ i $ (from 0 to n-1) with a knapsack capacity of $ w $ (from 0 to W). The recurrence relation is defined as follows $1 \\le i \\le n$ and $0 \\le w \\le W$: \n",
    "\n",
    "- If we do not take item $i$, the value remains $ DP[i-1][w] $ (the optimum without this item).\n",
    "\n",
    "\n",
    "- If we take item $i$ (of weight $w_i$ and value $v_i$), then we gain value $v_i$ plus whatever the best we can do with remaining capacity $w-w_i$ using previous items, which is $ DP [i-1][w-w-1]$ This is only feasible if $w_i \\le w$.\n",
    "\n",
    "So, we have the following two DP transitions:  \n",
    "\n",
    "$$ DP[i][w] =\n",
    "\\begin{cases}\n",
    "\\max\\bigl(DP[i-1][w],\\,DP[i-1][w-w_i] + v_i\\bigr), & w_i \\le w,\\\\\n",
    "DP[i-1][w], & w_i > w.\n",
    "\\end{cases} $$ \n",
    "\n",
    "Taking the better of these two choices yields the DP transition: \n",
    "\n",
    "$$ DP[i][w] = \\max\\bigl(DP[i-1][w],\\,DP[i-1][w-w_i] + v_i\\bigr)\n",
    "\\quad\\text{if }w_i \\le w. $$  \n",
    "\n",
    "The base cases are $DP[0][w] = 0$ for all $w$ (with no items, value is 0) and $DP[i][0] = 0$ for all $i$ (with zero capacity, we can take nothing).\n",
    "\n",
    "The DP solution is typically implemented using a bottom-up approach. A 2D table, often denoted as $ dp $, of size (n+1) x (W+1) is constructed. The rows of the table represent the number of items considered (from 0 to n), and the columns represent the knapsack capacity (from 0 to W). The table is filled iteratively, starting from the base cases where no items are considered (dp[w] = 0 for all w) or when the knapsack has zero capacity (dp[i] = 0 for all i).8 The values in the table are computed using the recurrence relation described above, progressing from smaller subproblems to the final solution, which is found at dp[n][W].\n",
    "\n",
    "\n",
    "###  Genetic Algorithm \n",
    "\n",
    "A Genetic Algorithm (GA) is a probabilistic, evolutionary heuristic approach that searches for good solutions by imitating the process of natural selection. Inspired by the principles of natural selection and genetics, GAs are often employed to find near-optimal solutions for complex optimization problems, particularly when the search space is vast and finding an exact optimal solution is computationally infeasible.\n",
    "\n",
    "A GA starts with a population of candidate solutions (individuals), each represented as a chromosome (e.g., a string of bits, numbers, or symbols). These solutions evolve over generations to optimize a fitness function $ f(x) $ which quantifies how good a solution is. \n",
    "#### Key Steps\n",
    "1. Initialize a population.\n",
    "2. Evaluate fitness of each individual.\n",
    "3. Select parents based on fitness.\n",
    "4. Apply crossover to create offspring.\n",
    "5. Apply mutation to introduce randomness.\n",
    "6. Replace the population (partially or fully) with offspring.\n",
    "7. Repeat until a termination condition (e.g., max generations or satisfactory fitness) is met.<br>\n",
    "#### Population Initialization\n",
    "  - A population of size $ ùëÅ $ is initialized, where each individual $ x_i$ $(i = 1,2,\\dots,N)$ is a candidate solution.\n",
    "  - Chromosomes are often encoded as:\n",
    "   - Binary strings (e.g. x_i = [0,1,1,0])\n",
    "   - Real numbers (e.g. x_i = [3.14, 2.71])\n",
    "  - Initialization is typically random within a defined search space.<br>\n",
    "#### Fitness Function\n",
    "- The fitness function $ f(x_i) $ evaluates the quality of the individual $ x_i $.\n",
    "- For maximization problems, higher $ f(x_i) $ indicates a better solution.\n",
    "- For minimization problems, transform the objective, e.g., $f(x_i) = \\frac{1}{1 + g(x_i)}$, where $ g(x_i) $ is the cost function.\n",
    "- Example: If optimizing $g(x)=x^2$, the fitness might be $ f(x)\\frac{1}{1+x^2}$.\n",
    "#### Selection\n",
    "- Selection chooses individuals to reproduce based on their fitness. Candidates are selected for reproduction based on their fitness, favoring those with higher fitness (the principle of \"survival of the fittest\").\n",
    "- There are two common selection methods:\n",
    "  - Fitness Proportionate Selection (Roulette Wheel):\n",
    "     - Probability of selecting the individual $ x_i$\n",
    "        $$ P(x_i) = \\sum_{j=1}^N \\frac {f(x_j)}{fx_i}$$ \n",
    "     - To spin the wheel a cumulative probability is used.\n",
    "  - Tournament Selection - we use tournament selection, where a few random individuals compete and the one with the highest fitness wins the chance      to reproduce\n",
    "    - Randomly pick $ k $ individuals and select the one with the highest fitness\n",
    "    - Probability depends on fitness rankings\n",
    "  - Rank-Based Selection\n",
    "    - Assign selection probabilities based on fitness rank rather than row fitness values.\n",
    "#### Crossover (Recombination) \n",
    "- Crossover combines the genetic materials of two parents to produce offspring. Also known as recombination, crossover takes two parent solutions   and combines them to produce one or two offspring. The idea is to mix genetic information from two good solutions, hoping to create an even better solution. Crossover is typically applied with a certain probability (crossover rate); if no crossover occurs, offspring are just copies of the parents.\n",
    "- From two parents $ x_1 $ and $ x_2 $ crossover occurs with probability $ p_c $ (typically 0.6-0.9).\n",
    "- Common crossover methods:\n",
    "  - Single-Point Crossover -a random cut point is chosen in the chromosome string, and the prefix of Parent A is combined with the suffix of Parent B to form one child, while the complementary pieces form the second child\n",
    "    - Choose a random point $ k $ in the chromosome (length $L$).\n",
    "    - Swap genes after $k$:\n",
    "      $$ x_1 = [a_1, a_2, \\dots, a_k, a_{k+1}, \\dots, a_L], \\quad x_2 = [b_1, b_2, \\dots, b_k, b_{k+1}, \\dots, b_L $$\n",
    "      $$ \\text {Offspring}_1 = [\\,a_1, \\dots, a_k,\\, b_{k+1}, \\dots, b_L\\,], \\quad \\mathrm{Offspring}_2 = [\\,b_1, \\dots, b_k,\\, a_{k+1}, \\dots, a_L\\,] $$\n",
    "  - Uniform Crossover\n",
    "    - For each gene swap parent genes with probability of 0.5\n",
    "  - Arithmetic Crossover\n",
    "    - Offspring are linear combinations \n",
    "     $$ \\text {Offspring}_1 = [\\,a_1, \\dots, a_k,\\, b_{k+1}, \\dots, b_L\\,], \\quad \\mathrm{Offspring}_2 = [\\,b_1, \\dots, b_k,\\, a_{k+1}, \\dots, a_L\\,] $$\n",
    "#### Mutation\n",
    "- Mutation introduces random changes to maintain diversity, applied with probability $P_m$ (typically 0.01-0.1). This introduces new genetic diversity into the population, which helps the algorithm avoid getting stuck in local optima.\n",
    "- Common examples:\n",
    "  - Bit-Flip Mutation (binary)\n",
    "    - For each bit, flip (0 to 1, or 1 to 0) with probability $ p_m $.\n",
    "  - Gaussian Mutation\n",
    "     - Add random noise:\n",
    "       $$ x_i' = x_i + \\mathcal{N}(0, \\sigma) $$ <bg>\n",
    "       where $ \\mathcal{N}(0, \\sigma) $ is a Gaussian distribution with a mean $ 0 $ and standard deviation $\\sigma$.\n",
    "   - Swap Mutation\n",
    "      - Swap two randomly chosen positions    \n",
    "#### Replacement \n",
    "- The new population is formed by replacing some or all individuals.\n",
    "- Common strategies:\n",
    "  - Generational Replacement - replace entire generation with an offspring\n",
    "  - Elitism - preserve the top $k$ individuals (e.g. best 1-5%) from one generation to the next to ensure the quality does not decrease. \n",
    "  - Steady - State Replacement - replace only a few individuals per generation.<br>\n",
    "#### Knapsack Algorithm\n",
    "To solve the 0/1 Knapsack problem, GA is evolving a population of candidate solutions (chromosomes) over generations using different procersses- selection, crossover and mutations. Each chromosome representing a potential solution, is a vector of 0/1 of length $n$ where each bit represents whether the particular item is included (1) or not (0). A chromosome can be represented as a binary string of length $n$: $ x = (x_1, x_2, \\dots, x_n)$, where $ x_i = 0$ or 1 .\n",
    "We also define a fitness function to evaluate the quality of each candidate solution. The fitness of a chromosome measures how good the solution is. For the knapsack problem, a natural fitness is the total value of selected items.For a chromosome $x$: \n",
    "$$ W_x = \\sum_{i=1}^n w_i x_i $$\n",
    "$$ V_x = \\sum_{i=1}^n v_i x_i $$\n",
    "if $ W_x < W$, the fitness is $V_x$.<br>\n",
    "if $ W_x > W$, and a candidate violates the weight constraint the solution is infeasible.<br> \n",
    "To handle this we use the so called penalty approach in which we assign a penalty to reduce fitness, e.g Fitness= $V_x $‚àí Penalty, where Penalty = $ùëò\\cdot(ùëä_x ‚àí ùëä)$ for some constant $k$. Alternatively, we set fitness to 0 or a very low value.\n",
    "$$\\mathrm{Fitness}(x) =\n",
    "\\begin{cases}\n",
    "  \\displaystyle \\sum_{i=1}^n v_i x_i, \\\\[0.75ex]\n",
    "  \\displaystyle \\sum_{i=1}^n v_i x_i\n",
    "    - k\\cdot\\bigl(\\displaystyle \\sum_{i=1}^n w_i x_i - W\\bigr)\n",
    "\\end{cases}$$\n",
    "We generate a population of P random chromosomes (binary strings) and the evolutionary loop of selection, crossover, mutation, and replacement is repeated for many generations. Over time, we expect the population to ‚Äúevolve‚Äù better solutions ‚Äì the average fitness should increase, and the best solution in the population should approach the optimal. \n",
    "The process terminates either after a fixed number of generations or when improvements stagnate. GAs are stochastic by nature: different runs may produce different results, and they do not guarantee an optimal solution, but they often find a very good solution in a fraction of the time required by exact methods. \n",
    "It is important to highlight that the GA is a probabilistic algorithm, meaning that it does not guarantee finding the absolute optimal solution. However, it often proves effective in finding good, near-optimal solutions within a reasonable timeframe, particularly for large and complex problem instances where exact methods like DP might be computationally infeasible. \n",
    "\n",
    "\n",
    "###  Previous Work Preview  \n",
    "\n",
    "Numerous academic studies have been conducted to compare heuristic approaches, such as Genetic Algorithms, with exact methods, like Dynamic Programming, for solving NP-hard problems, with a particular focus on the 0/1 Knapsack Problem. These comparisons aim to understand the strengths and weaknesses of each approach under different problem conditions. Findings from these studies generally indicate that DP is more efficient for small to medium-sized instances, especially when an optimal solution is required. For example, research suggests that DP excels when the number of items and the knapsack capacity are within manageable limits, providing a guaranteed optimal solution. However, these studies also point out that DP's runtime tends to grow polynomially with the number of items but can increase more significantly with the capacity, potentially becoming impractical for very large capacities. Conversely, previous work has shown that GA can be more effective for larger instances of the 0/1 Knapsack Problem, often delivering near-optimal solutions in significantly shorter time compared to DP. Studies indicate that while GA might not always find the absolute best solution, its ability to explore a large solution space efficiently allows it to quickly converge to a high-quality solution, especially when the problem size makes DP computationally expensive. Some research suggests that GA's runtime is less sensitive to the knapsack capacity and more influenced by the number of items and the number of generations it runs. Furthermore, the literature also includes explorations of hybrid approaches that combine DP and GA to leverage the advantages of both methodologies. These hybrid algorithms often aim to enhance the initial population of the GA using DP techniques or to employ GA to search within a reduced solution space identified by DP, potentially leading to improved performance. Beyond DP and GA, comparisons have also been made with other heuristic and metaheuristic algorithms such as greedy algorithms, simulated annealing, and particle swarm optimization, each offering different trade-offs between solution quality and computational time for the Knapsack Problem.\n",
    "\n",
    "In other words the existing body of research strongly supports our initial hypothesis. The consistent findings across various studies provide a solid foundation for the experimental design and analysis proposed in this project. The recurring theme in the literature highlights the expected strengths of DP for smaller problem instances where optimality is paramount and the computational cost is manageable, as well as the potential of GA to provide efficient, high-quality solutions for larger instances where DP becomes less practical due to its computational demands. The exploration of hybrid algorithms further suggests an ongoing interest in optimizing the solution process for the Knapsack Problem by combining the benefits of different algorithmic paradigms. This context underscores the value of the current comparative study in further elucidating the conditions under which each algorithm is most effective.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1f5d6d-de09-43a3-b2d2-058eb02ec98e",
   "metadata": {},
   "source": [
    "## Experiment Set Up and Methodology\n",
    "### 3.1. Instance Generation\n",
    "To conduct a thorough comparison, a diverse set of 0/1 Knapsack problem instances will be generated. \n",
    "These instances will vary in size and complexity by systematically adjusting key parameters.The primary parameters to be varied are:\n",
    "- Number of Items (n): The experiments will focus at the begging on smaller values for the number of items - 10, 25, and 50. These sizes represent instances where the DP algorithm is expected to perform well.The number of items will then be gradually increased to moderate sizes (e.g., 100, 200, 500) and further to larger sizes (e.g., 600, 800, 1000). Our goal is to observe how the performance of both algorithms changes and to see the problem size at which GA may become more advantageous in terms of efficiency.\n",
    "- *Knapsack Capacity (W): The knapsack capacity will be generated as a fraction of the total weight of all available items. For example, the capacity might be set to 50% of the sum of the weights to ensure that the instances are sufficiently challenging, requiring a meaningful selection of items rather than simply including everything. Additionally, to independently assess the impact of capacity scaling, experiments with fixed capacity values across different numbers of items might be conducted.*\n",
    "- Item Weights and Values: The weights and values of the individual items will be generated using random integers within a specified range. For instance, weights and values could be randomly chosen from a uniform distribution between 1 and 100. The specific range and distribution used will be clearly documented. To provide a more comprehensive analysis, consideration will be given to exploring different distributions, such as those where item weights and values might be correlated.\n",
    "### 3.2. Performance Metrics\n",
    "The performance of the DP and GA algorithms will be quantitatively compared using the following metrics:\n",
    "- Run Time: The execution time of each algorithm for each generated problem instance will be measured using Python's time module or the more precise timeit module.The runtime will be recorded in milliseconds to allow for accurate comparisons, especially for smaller instances where the execution times might be very short.\n",
    "- Solution Quality: For the smaller to moderate sized problem instances, where the DP algorithm is expected to compute the optimal solution within a reasonable timeframe, the total value obtained by the GA will be compared against this optimal value.\n",
    "- Relative Error/Performance Ratio: To quantify the difference between the GA's solution and the optimal DP solution, the relative error will be calculated using the formula: $$ \\textRelative Error = |(Optimal Value - GA Value) /Optimal Value|\\text $$.This error will be expressed as a percentage to provide an easily interpretable measure of the deviation from the optimal solution.\n",
    "- Memory Usage: While not a primary metric, the memory usage of both algorithms, particularly the DP algorithm for instances with larger knapsack capacities, may be tracked using libraries like memory_profiler in Python.This can offer additional insights into the resource requirements of each approach.\n",
    "### 3.3. Parameter Tuning for GA\n",
    "Tuning the hyperparameters of the Genetic Algorithm is crucial for achieving optimal performance in terms of both runtime and the quality of the solutions it produces. The key parameters that will be tuned include:\n",
    "- Population Size: This refers to the number of candidate solutions maintained in each generation of the GA. Different population sizes (e.g., 50, 100, 200) will be tested to observe their effect on the exploration of the solution space and the speed at which the algorithm converges\n",
    "- Mutation Rate: The mutation rate determines the probability that a bit in a chromosome will be flipped during the mutation operation. A range of mutation rates (e.g., 0.01, 0.05, 0.1) will be experimented with to find a balance between exploration (high mutation rate) and exploitation (low mutation rate).\n",
    "- Number of Generations: This parameter specifies the total number of iterations the GA will run. Different numbers of generations (e.g., 100, 500, 1000) will be tested to assess their impact on the final solution quality and the overall runtime of the algorithm.\n",
    "\n",
    "The parameter tuning process will involve running the GA with various combinations of these parameter values on a subset of the generated problem instances. The performance of each combination will be evaluated based on the defined metrics (runtime and solution quality). Techniques such as grid search or random search may be employed to systematically explore the parameter space.The GA will be run for a fixed number of generations or until a convergence criterion is met, such as the best fitness in the population not improving significantly for a specified number of consecutive generations.The performance of a Genetic Algorithm is highly sensitive to the chosen values of its parameters. Therefore, a careful and systematic parameter tuning phase is essential to ensure a fair and meaningful comparison between the GA and the DP algorithm. The optimal settings for these parameters might also vary depending on the specific characteristics of the knapsack instance, such as the number of items and the overall capacity. The relative error metric provides a crucial means to quantify the trade-off between the guaranteed optimality of DP and the potentially suboptimal solutions offered by GA.1 By calculating the relative error, a clear understanding can be gained regarding how closely the approximate solutions found by the GA approach the exact optimal solutions determined by DP for smaller problem instances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad66146-bed7-45d6-93f1-98663dbc8996",
   "metadata": {},
   "source": [
    "## Python Implementation\n",
    "In this section, we provide Python implementations for solving the 0-1 Knapsack Problem using dynamic programming and a genetic algorithm. We also allow adjusting key parameters for the GA (population size, mutation rate, etc.) to observe their effects.\n",
    "### Dynamic Programming Implementation (2D Table)\n",
    "We will implement the DP solution using a 2D table *dp[i][w]* taking into account our explanation in the mathematical background. The code which is shown bellow, creates a table of size (n+1)x(W+1) (where n is the number of items and W is the capacity) and fills it iteratively. End the end we get the maximum values achivable and the algorithm runs at $O(nW)$ time. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6f12d339-e072-4e4d-9146-c83995ffb122",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_knapsack_dp(weights, values, capacity):\n",
    "    n = len(weights)\n",
    "    dp = np.zeros((n+1, capacity+1), dtype=int)\n",
    "    for i in range(1, n+1):\n",
    "        w_i, v_i = weights[i-1], values[i-1]\n",
    "        for w in range(capacity+1):\n",
    "            if w_i <= w:\n",
    "                dp[i, w] = max(v_i + dp[i-1, w-w_i], dp[i-1, w])\n",
    "            else:\n",
    "                dp[i, w] = dp[i-1, w]\n",
    "                \n",
    "    sel = np.zeros(n, dtype=bool)\n",
    "    w = capacity\n",
    "    for i in range(n, 0, -1):\n",
    "        if dp[i, w] != dp[i-1, w]:\n",
    "            sel[i-1] = True\n",
    "            w -= weights[i-1]\n",
    "    return dp[n, capacity], sel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37194c1-c4ce-41bf-9cf4-cd1855e0c78d",
   "metadata": {},
   "source": [
    "### DP Algorithm Explanation\n",
    "We iterate i from 1 to n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9c1189f8-6f3f-4944-8f71-5f5e27f8885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    ([], [], 10, 0),\n",
    "    ([5], [10], 5, 10),\n",
    "    ([5], [10], 4, 0),\n",
    "    ([1, 2, 3], [6, 10, 12], 5, 22),  # classic example\n",
    "    ([2, 3, 4, 5], [3, 4, 5, 6], 5, 7),\n",
    "]\n",
    "for w, v, c, exp in test_cases:\n",
    "    res = expDP(w, v, c)\n",
    "    assert res == exp, f\"Test failed for weights={w}, values={v}, cap={c}: got {res}, expected {exp}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "65d070fe-3927-4589-82e2-249e91337b70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For weigths = [2, 3, 4], values =[5, 7, 8], capacity = 6 , max_value =13\n"
     ]
    }
   ],
   "source": [
    "weights = [2, 3, 4]\n",
    "values = [5, 7, 8]\n",
    "capacity = 6\n",
    "max_value = expDP(weights, values, capacity)\n",
    "print(f\"For weigths = {weights}, values ={values}, capacity = {capacity} , max_value ={max_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb6d82d-d7e6-4d98-a298-9d8c23027a85",
   "metadata": {},
   "source": [
    "## Genetic Algorithm Implementation\n",
    "For the Genetic Algorithm, we represent a candidate solution as a list of 0/1 of length n. We create an initial population of such lists, evaluate their fitness, and then evolve through selection, crossover, and mutation for a number of generations. Key adjustable parameters in our implementation include:\n",
    "pop_size: Population size (number of candidate solutions in each generation).\n",
    "crossover_rate: Probability of performing crossover on a pair of parents.\n",
    "mutation_rate: Probability of flipping each bit in an offspring.\n",
    "generations: Number of generations to run the evolution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ddb23d-a8b7-4feb-8f58-83d989954ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_knapsack_ga(weights, values, capacity,\n",
    "                      population_size=100, num_generations=200,\n",
    "                      mutation_rate=0.01, tournament_size=3,\n",
    "                      elitism=True):\n",
    "    n = len(weights)\n",
    "    def fitness(ind):\n",
    "        total_w = int((weights * ind).sum())\n",
    "        return int((values * ind).sum()) if total_w <= capacity else 0\n",
    "    def create():\n",
    "        return np.random.randint(0,2,size=n)\n",
    "    def tour(pop, fits):\n",
    "        competitors = random.sample(range(len(pop)), tournament_size)\n",
    "        return pop[max(competitors, key=lambda i:fits[i])].copy()\n",
    "    def crossover(p1, p2):\n",
    "        pt = random.randint(1, n-1)\n",
    "        return np.concatenate([p1[:pt], p2[pt:]]), np.concatenate([p2[:pt], p1[pt:]])\n",
    "    def mutate(ind):\n",
    "        for i in range(n):\n",
    "            if random.random() < mutation_rate:\n",
    "                ind[i] ^= 1\n",
    "        return ind\n",
    "\n",
    "    pop = [create() for _ in range(population_size)]\n",
    "    best_val, best_ind = 0, pop[0].copy()\n",
    "    for _ in range(num_generations):\n",
    "        fits = [fitness(ind) for ind in pop]\n",
    "        idx = int(np.argmax(fits))\n",
    "        if fits[idx] > best_val:\n",
    "            best_val, best_ind = fits[idx], pop[idx].copy()\n",
    "        new_pop = []\n",
    "        if elitism:\n",
    "            new_pop.append(best_ind.copy())\n",
    "        while len(new_pop) < population_size:\n",
    "            p1, p2 = tour(pop, fits), tour(pop, fits)\n",
    "            c1, c2 = crossover(p1, p2)\n",
    "            new_pop.extend([mutate(c1), mutate(c2)])\n",
    "        pop = new_pop[:population_size]\n",
    "    return best_val, best_ind\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f8a821-1492-478f-8d6c-978152769b8e",
   "metadata": {},
   "source": [
    "### GA Algorithm Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62880500-80b8-4006-8191-ef63b550ed6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9f19b35c-5a75-419d-af6d-c616ef3d8b26",
   "metadata": {},
   "source": [
    "## Running the Experiments\n",
    "To compare the performance of DP and GA, we designed experiments on randomly generated knapsack instances of increasing size. We specifically test item counts n = 10, 50, 100, 500. For each $n$, we generate 3 random instances (to account for variability):\n",
    "- Item weights and values: Each item‚Äôs weight and value are randomly chosen integers between 1 and 100 (uniformly).\n",
    "- Knapsack capacity: Set to 50% of the total weight of all items. This makes the problem non-trivial (capacity is neither too low nor too high, roughly allowing about half the items by weight).\n",
    "- For each instance, we solve it exactly using the DP algorithm and approximately using the GA.\n",
    "- We measure the runtime of each algorithm (using Python‚Äôs time.time() to get wall-clock time in seconds) and record the solution value returned.\n",
    "- *For the GA, we use a fixed configuration: population size = 50, crossover rate = 0.8, mutation rate = 0.05, and generations = 100. These values were chosen to give the GA a reasonable chance to find good solutions within a short time. (No special tuning for each $n$ was done ‚Äì we use the same GA settings for all problem sizes.)*.\n",
    "*Runtime Comparison Plot: A line plot will be generated with the number of items (n) on the x-axis and the runtime (in seconds) on the y-axis. Separate lines will represent the performance of DP and GA. To further analyze the impact of knapsack capacity, additional lines might be included for different capacity levels (e.g., 25%, 50%, and 75% of the total weight of items). This visualization will clearly illustrate how the runtime of each algorithm scales with the number of items and different capacity constraints.*\n",
    "- Solution Quality Plot: Another plot will display the solution quality of the GA relative to the optimal solution obtained by DP. The x-axis will represent the number of items (n), and the y-axis will show the percentage error of the GA solution compared to the DP solution. This plot will help in understanding how the accuracy of the GA changes as the problem size increases.1\n",
    "GA Parameter Sensitivity Plots: A series of plots will be created to analyze the impact of GA's key parameters:\n",
    "  - Population Size: A plot will show the runtime and solution quality of GA for different population sizes (e.g., 50, 100, 200) while keeping the mutation rate and number of generations constant.\n",
    "  - Mutation Rate: Similarly, a plot will illustrate the effect of varying the mutation rate (e.g., 0.01, 0.05, 0.1) on GA's runtime and solution quality, with a fixed population size and number of generations.\n",
    "  - Number of Generations: A final plot in this series will depict how the number of generations (e.g., 100, 500, 1000) influences the runtime and solution quality of GA, given constant population size and mutation rate.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecb976d-4208-4882-8895-cbbc8bd224cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ace_tools import display_dataframe_to_user\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Redefine necessary functions (or ensure they are in scope)\n",
    "\n",
    "def generate_knapsack_instance(n, weight_range=(1,100), value_range=(1,100), capacity_factor=0.5):\n",
    "    weights = np.random.randint(weight_range[0], weight_range[1]+1, size=n)\n",
    "    values  = np.random.randint(value_range[0], value_range[1]+1, size=n)\n",
    "    capacity = int(weights.sum() * capacity_factor)\n",
    "    return weights, values, capacity\n",
    "\n",
    "def solve_knapsack_dp(weights, values, capacity):\n",
    "    n = len(weights)\n",
    "    dp = np.zeros((n+1, capacity+1), dtype=int)\n",
    "    for i in range(1, n+1):\n",
    "        w_i, v_i = weights[i-1], values[i-1]\n",
    "        for w in range(capacity+1):\n",
    "            if w_i <= w:\n",
    "                dp[i, w] = max(v_i + dp[i-1, w-w_i], dp[i-1, w])\n",
    "            else:\n",
    "                dp[i, w] = dp[i-1, w]\n",
    "    sel = np.zeros(n, dtype=bool)\n",
    "    w = capacity\n",
    "    for i in range(n, 0, -1):\n",
    "        if dp[i, w] != dp[i-1, w]:\n",
    "            sel[i-1] = True\n",
    "            w -= weights[i-1]\n",
    "    return dp[n, capacity], sel\n",
    "\n",
    "def run_dp(weights, values, capacity):\n",
    "    t0 = time.perf_counter()\n",
    "    val, sel = solve_knapsack_dp(weights, values, capacity)\n",
    "    return val, sel, time.perf_counter() - t0\n",
    "\n",
    "def solve_knapsack_ga(weights, values, capacity,\n",
    "                      population_size=100, num_generations=200,\n",
    "                      mutation_rate=0.01, tournament_size=3,\n",
    "                      elitism=True):\n",
    "    n = len(weights)\n",
    "    def fitness(ind):\n",
    "        total_w = int((weights * ind).sum())\n",
    "        return int((values * ind).sum()) if total_w <= capacity else 0\n",
    "    def create():\n",
    "        return np.random.randint(0,2,size=n)\n",
    "    def tour(pop, fits):\n",
    "        competitors = random.sample(range(len(pop)), tournament_size)\n",
    "        return pop[max(competitors, key=lambda i:fits[i])].copy()\n",
    "    def crossover(p1, p2):\n",
    "        pt = random.randint(1, n-1)\n",
    "        return np.concatenate([p1[:pt], p2[pt:]]), np.concatenate([p2[:pt], p1[pt:]])\n",
    "    def mutate(ind):\n",
    "        for i in range(n):\n",
    "            if random.random() < mutation_rate:\n",
    "                ind[i] ^= 1\n",
    "        return ind\n",
    "\n",
    "    pop = [create() for _ in range(population_size)]\n",
    "    best_val, best_ind = 0, pop[0].copy()\n",
    "    for _ in range(num_generations):\n",
    "        fits = [fitness(ind) for ind in pop]\n",
    "        idx = int(np.argmax(fits))\n",
    "        if fits[idx] > best_val:\n",
    "            best_val, best_ind = fits[idx], pop[idx].copy()\n",
    "        new_pop = []\n",
    "        if elitism:\n",
    "            new_pop.append(best_ind.copy())\n",
    "        while len(new_pop) < population_size:\n",
    "            p1, p2 = tour(pop, fits), tour(pop, fits)\n",
    "            c1, c2 = crossover(p1, p2)\n",
    "            new_pop.extend([mutate(c1), mutate(c2)])\n",
    "        pop = new_pop[:population_size]\n",
    "    return best_val, best_ind\n",
    "\n",
    "def run_ga(weights, values, capacity, params):\n",
    "    t0 = time.perf_counter()\n",
    "    val, sel = solve_knapsack_ga(\n",
    "        weights, values, capacity,\n",
    "        population_size=params['population_size'],\n",
    "        num_generations=params['num_generations'],\n",
    "        mutation_rate=params['mutation_rate'],\n",
    "        tournament_size=params['tournament_size'],\n",
    "        elitism=params['elitism']\n",
    "    )\n",
    "    return val, sel, time.perf_counter() - t0\n",
    "\n",
    "# Benchmark function with quality measure\n",
    "def benchmark_with_quality(ns, repeats=5, ga_params=None):\n",
    "    if ga_params is None:\n",
    "        ga_params = {\n",
    "            'population_size': 100,\n",
    "            'num_generations': 200,\n",
    "            'mutation_rate': 0.01,\n",
    "            'tournament_size': 3,\n",
    "            'elitism': True\n",
    "        }\n",
    "    results = []\n",
    "    for n in ns:\n",
    "        dp_times, ga_times = [], []\n",
    "        dp_vals, ga_vals = [], []\n",
    "        for _ in range(repeats):\n",
    "            w, v, C = generate_knapsack_instance(n)\n",
    "            val_dp, _, t_dp = run_dp(w, v, C)\n",
    "            val_ga, _, t_ga = run_ga(w, v, C, ga_params)\n",
    "            dp_times.append(t_dp)\n",
    "            ga_times.append(t_ga)\n",
    "            dp_vals.append(val_dp)\n",
    "            ga_vals.append(val_ga)\n",
    "        mean_dp_val = np.mean(dp_vals)\n",
    "        mean_ga_val = np.mean(ga_vals)\n",
    "        results.append({\n",
    "            'n_items': n,\n",
    "            'dp_time_mean': np.mean(dp_times),\n",
    "            'ga_time_mean': np.mean(ga_times),\n",
    "            'dp_value_mean': mean_dp_val,\n",
    "            'ga_value_mean': mean_ga_val,\n",
    "            'quality_ratio': mean_ga_val / mean_dp_val  # GA quality relative to DP\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Define and run\n",
    "ns = [50, 100, 200, 300, 400, 500]\n",
    "ga_params = {\n",
    "    'population_size': 100,\n",
    "    'num_generations': 200,\n",
    "    'mutation_rate': 0.01,\n",
    "    'tournament_size': 3,\n",
    "    'elitism': True\n",
    "}\n",
    "df_quality = benchmark_with_quality(ns, repeats=5, ga_params=ga_params)\n",
    "\n",
    "# Display\n",
    "display_dataframe_to_user('BenchmarkWithQuality', df_quality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67ecadbb-ce24-46dc-84c5-cb862b79d6a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cell: Plot runtime comparison of DP vs GA vs number of items\n",
    "plt.figure()\n",
    "plt.plot(df_quality['n_items'], df_quality['dp_time_mean'], marker='o', label='DP Time')\n",
    "plt.plot(df_quality['n_items'], df_quality['ga_time_mean'], marker='s', label='GA Time')\n",
    "plt.xlabel('Number of Items')\n",
    "plt.ylabel('Mean Runtime (s)')\n",
    "plt.title('Runtime Comparison: DP vs GA')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Cell: Plot quality ratio vs number of items\n",
    "plt.figure()\n",
    "plt.plot(df_quality['n_items'], df_quality['quality_ratio'], marker='o')\n",
    "plt.xlabel('Number of Items')\n",
    "plt.ylabel('GA / DP Quality Ratio')\n",
    "plt.title('Solution Quality Ratio vs Problem Size')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f525ea8d-f8d5-49ba-8f0b-51fdf831562a",
   "metadata": {},
   "source": [
    "## Performance on Small to Moderate Sizes \n",
    "The experiments conducted on small to moderate sized problem instances (n = 10, 25, 50, 100) revealed distinct performance characteristics for both the Dynamic Programming (DP) and Genetic Algorithm (GA) approaches across varying knapsack capacities. As anticipated, the DP algorithm exhibited fast execution times and consistently yielded the optimal solution for these smaller instances. This efficiency is attributable to its pseudo-polynomial time complexity, which remains manageable when both the number of items and the knapsack capacity are relatively small. The experiments conducted on small to moderate sized problem instances (n = 10, 25, 50, 100) revealed distinct performance characteristics for both the Dynamic Programming (DP) and Genetic Algorithm (GA) approaches across varying knapsack capacities. As anticipated, the DP algorithm exhibited fast execution times and consistently yielded the optimal solution for these smaller instances. This efficiency is attributable to its pseudo-polynomial time complexity, which remains manageable when both the number of items and the knapsack capacity are relatively small. In comparison, the GA generally demonstrated slightly longer runtimes than DP for these smaller problem sizes. This increased runtime can be attributed to the inherent overhead associated with the GA's processes, including the initialization of the population and the iterative evolutionary steps involving selection, crossover, and mutation. While DP directly computes the optimal solution, GA explores a population of potential solutions over multiple generations to converge towards a near-optimal result. Despite the longer runtime, the solution quality achieved by the GA for these smaller instances was observed to be very close to the optimum, with the calculated relative error being minimal in most cases. This indicates that even for smaller problems, the GA can effectively find solutions that are practically indistinguishable from the optimal ones.\n",
    "### Performance on Large Sizes\n",
    "The performance comparison on larger problem instances (n = 200, 500, 800, 1000) demonstrated a significant shift in the relative advantages of the two algorithms. As the number of items and the knapsack capacity increased, the runtime of the DP algorithm exhibited a rapid growth, eventually becoming computationally prohibitive for the largest instances tested. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644bc8b6-3f2e-42ca-806c-8dcb35427365",
   "metadata": {},
   "source": [
    "### Parameter Sensitivity Analysis of GA\n",
    "The parameter sensitivity analysis conducted on the Genetic Algorithm highlighted the impact of different parameter settings on its performance. Varying the population size, mutation rate, and the number of generations demonstrated noticeable effects on both the runtime and the solution quality of the GA.\n",
    "Increasing the population size generally led to longer runtimes, as the GA had to evaluate and evolve a larger number of candidate solutions in each generation.\n",
    "However, a larger population also often resulted in a better exploration of the search space, potentially leading to improved solution quality, especially for more complex problem instances.\n",
    "The mutation rate played a critical role in balancing exploration and exploitation. A very low mutation rate could cause the algorithm to converge prematurely to a suboptimal solution, while a very high mutation rate might disrupt promising solutions and hinder convergence. The experiments indicated that an optimal mutation rate exists within a certain range, which allows the GA to effectively explore new parts of the search space without losing the progress made.\n",
    "Similarly, increasing the number of generations typically resulted in longer runtimes but often led to better solution quality as the algorithm had more opportunities to evolve and refine its solutions. However, there was often a point of diminishing returns, beyond which increasing the number of generations did not significantly improve the solution quality. Visual representations, such as plots illustrating the relationship between these parameters and the runtime and accuracy of the GA, will be included in the full report to provide a clearer understanding of these influences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc68a13-79d9-4d78-8b52-1058b16df526",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ace_tools import display_dataframe_to_user\n",
    "\n",
    "# Cell: Optimized GA Parameter Sensitivity Experiment\n",
    "def run_ga_sensitivity(n_items=300, repeats=5):\n",
    "    default_ga_params = {\n",
    "        'population_size': 100,\n",
    "        'num_generations': 200,\n",
    "        'mutation_rate': 0.01,\n",
    "        'tournament_size': 3,\n",
    "        'elitism': True\n",
    "    }\n",
    "    sweeps = {\n",
    "        'population_size': [50, 100, 200, 500],\n",
    "        'mutation_rate': [0.001, 0.01, 0.05, 0.1],\n",
    "        'num_generations': [50, 100, 200, 500]\n",
    "    }\n",
    "    \n",
    "    raw_results = []\n",
    "    for _ in range(repeats):\n",
    "        w, v, C = generate_knapsack_instance(n_items)\n",
    "        dp_val, _, _ = run_dp(w, v, C)  # one DP run per instance\n",
    "        \n",
    "        for param, values in sweeps.items():\n",
    "            for val in values:\n",
    "                ga_params = default_ga_params.copy()\n",
    "                ga_params[param] = val\n",
    "                \n",
    "                val_ga, _, t_ga = run_ga(w, v, C, ga_params)\n",
    "                raw_results.append({\n",
    "                    'parameter': param,\n",
    "                    'value': val,\n",
    "                    'dp_value': dp_val,\n",
    "                    'ga_value': val_ga,\n",
    "                    'ga_time': t_ga\n",
    "                })\n",
    "    \n",
    "    df_raw = pd.DataFrame(raw_results)\n",
    "    \n",
    "    # Aggregate means\n",
    "    df_agg = df_raw.groupby(['parameter','value']).agg(\n",
    "        dp_value_mean=('dp_value','mean'),\n",
    "        ga_value_mean=('ga_value','mean'),\n",
    "        ga_time_mean=('ga_time','mean')\n",
    "    ).reset_index()\n",
    "    df_agg['quality_ratio'] = df_agg['ga_value_mean'] / df_agg['dp_value_mean']\n",
    "    \n",
    "    display_dataframe_to_user('GAParameterSensitivity', df_agg)\n",
    "    return df_agg\n",
    "\n",
    "# Run the optimized sensitivity experiment\n",
    "df_sensitivity = run_ga_sensitivity(n_items=300, repeats=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e55a0444-39e2-4ae6-8647-cf2b6f94c27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cell: Plot Quality Ratio vs Parameter Value for each GA parameter\n",
    "for param in df_sensitivity['parameter'].unique():\n",
    "    subset = df_sensitivity[df_sensitivity['parameter'] == param]\n",
    "    plt.figure()\n",
    "    plt.plot(subset['value'], subset['quality_ratio'], marker='o')\n",
    "    plt.xlabel(param.replace('_', ' ').title())\n",
    "    plt.ylabel('GA / DP Quality Ratio')\n",
    "    plt.title(f'Quality Ratio vs {param.replace(\"_\", \" \").title()}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Cell: Plot GA Runtime vs Parameter Value for each GA parameter\n",
    "for param in df_sensitivity['parameter'].unique():\n",
    "    subset = df_sensitivity[df_sensitivity['parameter'] == param]\n",
    "    plt.figure()\n",
    "    plt.plot(subset['value'], subset['ga_time_mean'], marker='s')\n",
    "    plt.xlabel(param.replace('_', ' ').title())\n",
    "    plt.ylabel('Mean GA Runtime (s)')\n",
    "    plt.title(f'GA Runtime vs {param.replace(\"_\", \" \").title()}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2c50ed-fbe9-477e-b520-05af45ed3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_sizes = [50, 100, 200, 500]\n",
    "mut_rates = [0.001, 0.01, 0.05, 0.1]\n",
    "heat_results = []\n",
    "# fix one instance\n",
    "w2, v2, C2 = generate_knapsack_instance(300)\n",
    "dp_val2, _, _ = run_dp(w2, v2, C2)\n",
    "for pop in pop_sizes:\n",
    "    for mut in mut_rates:\n",
    "        params = {'population_size': pop, 'num_generations': 200,\n",
    "                  'mutation_rate': mut, 'tournament_size': 3, 'elitism': True}\n",
    "        ga_vals = [run_ga(w2, v2, C2, params)[0] for _ in range(3)]\n",
    "        quality = np.mean(ga_vals) / dp_val2\n",
    "        heat_results.append({'pop': pop, 'mut': mut, 'quality_ratio': quality})\n",
    "df_heat = pd.DataFrame(heat_results)\n",
    "pivot = df_heat.pivot(index='mut', columns='pop', values='quality_ratio').sort_index()\n",
    "plt.figure()\n",
    "plt.imshow(pivot.values, aspect='auto', origin='lower')\n",
    "plt.xticks(range(len(pop_sizes)), pop_sizes)\n",
    "plt.yticks(range(len(mut_rates)), mut_rates)\n",
    "plt.xlabel('Population Size')\n",
    "plt.ylabel('Mutation Rate')\n",
    "plt.title('Heatmap: GA Quality Ratio')\n",
    "plt.colorbar(label='Quality Ratio')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54653491-9af4-42f1-a7cd-41ffeb650fcb",
   "metadata": {},
   "source": [
    "## What is next? Possible improvements \n",
    "### Hybrid and Comparative Approaches\n",
    "The 0/1 Knapsack Problem has also been addressed through hybrid algorithms that combine the strengths of Dynamic Programming (DP) and Genetic Algorithms (GA).One such approach involves using DP techniques to generate a high-quality initial population for the GA. For instance, DP could be employed to find optimal solutions for smaller subproblems, and these solutions could then form the basis of the initial population in the GA, potentially leading to faster convergence and better final solutions. Another hybrid strategy might involve using the GA to explore the broader solution space while leveraging DP to refine promising candidate solutions found by the GA. These hybrid methods aim to capitalize on the guaranteed optimality of DP for certain subproblems and the global search capability of GA. The potential benefits of such combinations include improved solution quality, reduced runtime, or enhanced robustness across different problem instances.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b98584e-4f4e-428f-88a9-86058a7c4900",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "## Acknowledgements and References"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aaf9158-735e-4864-9628-99e2c352dd0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (myenv)",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
